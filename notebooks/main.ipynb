{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194f6753-1db1-4cbd-b141-e2c680564455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, copy, time, math, random, numbers, itertools, tqdm, importlib, re\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import rasterio\n",
    "import torch\n",
    "\n",
    "from sklearn import metrics\n",
    "from skimage import transform as trans\n",
    "from pathlib import Path\n",
    "from collections.abc import Sequence\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.ndimage import rotate\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7056708e-0e49-4e4d-9d75-b65b250b791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "sys.path.insert(0, module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ba94f1-2576-4bcc-bf6e-ce69f837f844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_dataset import *\n",
    "from utils import *\n",
    "from models.unet import *\n",
    "from model_compiler import *\n",
    "from accuracy_metric2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5101f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chips_multi_filtered\n",
    "# [\"v_flip\", \"h_flip\", \"d_flip\", \"rotate\", \"resize\", \"shift_brightness\"]\n",
    "# \"/home/data/chkpt/Unet_final_state.pth\"\n",
    "\n",
    "\n",
    "config = {\n",
    "    # Custom dataset params\n",
    "    \"src_dir\": \"/home/data\",\n",
    "    \"train_dataset_name\": \"chips_filtered\",\n",
    "    \"train_csv_path\": \"/home/workdir/train_ids.csv\",\n",
    "    \"val_csv_path\": \"/home/workdir/val_ids.csv\",\n",
    "    \"split_ratio\": 0.8,\n",
    "    \"apply_normalization\": True,\n",
    "    \"normal_strategy\": \"z_value\",\n",
    "    \"stat_procedure\": \"gpb\",\n",
    "    \"global_stats\": {\n",
    "        \"min\": np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0] * 3),\n",
    "        \"max\": np.array([300.0, 350.0, 500.0, 800.0, 900.0, 850.0] * 3),\n",
    "        \"mean\": np.array([\n",
    "            377.9830716, 653.4058739, 702.1834053, \n",
    "            2573.849324, 2344.568628, 1432.163695\n",
    "        ] * 3),\n",
    "        \"std\": np.array([\n",
    "            171.5116587, 215.5407551, 364.3545396, \n",
    "            686.5730746, 769.6448444, 675.9192684\n",
    "        ] * 3)\n",
    "    },\n",
    "    \"transformations\": [\n",
    "        \"v_flip\", \"h_flip\", \"d_flip\", \n",
    "        \"rotate\", \"resize\", \"shift_brightness\"\n",
    "    ],\n",
    "    \"aug_params\": {\n",
    "        \"scale_factor\": (0.75, 1.3),\n",
    "        \"rotation_degree\": (-180, -90, 90, 180),\n",
    "        \"bshift_gamma_range\": (0.2, 2),\n",
    "        \"bshift_subs\": (6, 6, 6),\n",
    "        \"patch_shift\": True\n",
    "    },\n",
    "    # DataLoader\n",
    "    \"train_BatchSize\": 10,\n",
    "    \"val_test_BatchSize\": 1,\n",
    "    # Model initialization params\n",
    "    \"n_classes\": 14,\n",
    "    \"input_channels\": 18,\n",
    "    \"filter_config\": (64, 128, 256, 512, 1024, 1024),\n",
    "    \"use_skipAtt\": False,\n",
    "    \"train_dropout_rate\": 0.15,\n",
    "    # Model compiler params\n",
    "    \"working_dir\": \"/home/workdir\",\n",
    "    \"out_dir\": \"/home/workdir/output2\",\n",
    "    \"class_mapping\": {\n",
    "        0:\"Unknown\", 1: \"Shrubland\", 2: \"Grassland/Pasture\", \n",
    "        3: \"Forest\", 4: \"Corn\", 5:\"Soybeans\", 6: \"Wetlands\",\n",
    "        7: \"Developed\", 8:\"Open Water\", 9: \"Winter Wheat\", \n",
    "        10: \"Other Hay/Non Alfalfa\", 11: \"Alfalfa\",\n",
    "        12: \"Fallow/Idle Cropland\", 13: \"Barren\", \n",
    "        14: \"Cotton\", 15: \"Sorghum\", 16:\"Other\"\n",
    "    },\n",
    "    \"gpuDevices\": [0],\n",
    "    \"init_type\": \"kaiming\",\n",
    "    \"params_init\": None,\n",
    "    \"freeze_params\": None,\n",
    "    # Model fitting\n",
    "    \"epochs\": 50,\n",
    "    \"optimizer\": \"sam\",\n",
    "    \"LR\": 0.01,\n",
    "    \"LR_policy\": \"PolynomialLR\",\n",
    "    \"criterion\": (\"TverskyFocalLoss(weight=[0.023839441, 0.141533235, 0.115825893,\"\n",
    "              \"0.14021708, 0.124164449, 0.095742291, 0.080351933, 0.029785156,\"\n",
    "              \"0.042391139, 0.029357087, 0.03725627, 0.033638325, 0.001587225,\"\n",
    "              \"0.020611555, 0.017460882, 0.066238038], ignore_index=0)\"),\n",
    "    \"momentum\": 0.95,\n",
    "    \"resume\": False,\n",
    "    \"resume_epoch\": None,\n",
    "    \"lr_prams\": {\n",
    "        # StepLR & MultiStepLR\n",
    "        \"step_size\": 3,\n",
    "        \"milestones\": [5, 10, 20, 35, 50, 70, 90],\n",
    "        \"gamma\": 0.98,\n",
    "        # ReduceLROnPlateau\n",
    "        \"mode\": \"min\",\n",
    "        \"factor\": 0.8,\n",
    "        \"patience\": 3,\n",
    "        \"threshold\": 0.0001,\n",
    "        \"threshold_mode\": \"rel\",\n",
    "        \"min_lr\": 3e-6,\n",
    "        # PolynomialLR\n",
    "        \"max_decay_steps\": 50,\n",
    "        \"min_learning_rate\": 1e-5,\n",
    "        \"power\": 0.85,\n",
    "        # CyclicLR\n",
    "        \"base_lr\": 3e-5,\n",
    "        \"max_lr\": 0.01,\n",
    "        \"step_size_up\": 1100,\n",
    "        \"mode\": \"triangular\"\n",
    "    },\n",
    "    # Model accuracy evaluation\n",
    "    \"val_metric_fname\" : \"validate_metrics_global_V2.csv\"   \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b00efdb-8a76-4271-b77d-fb95faac315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1-1\n",
    "# Read the input dataset for training, pre-process it, and generate a 'torch.utils.Dataset' object \n",
    "# that can be accepted to get loaded into the model.\n",
    "train_dataset = CropData(src_dir=config[\"src_dir\"],\n",
    "                         usage=\"train\",\n",
    "                         dataset_name=config[\"train_dataset_name\"],\n",
    "                         csv_path=config[\"train_csv_path\"],\n",
    "                         apply_normalization=config[\"apply_normalization\"],\n",
    "                         normal_strategy=config[\"normal_strategy\"],\n",
    "                         stat_procedure=config[\"stat_procedure\"],\n",
    "                         global_stats=config[\"global_stats\"],\n",
    "                         trans=config[\"transformations\"], \n",
    "                         **config[\"aug_params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfc9896-e31f-4b6d-99d4-a336ffd4326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN\n",
    "# Making a subset of the train dataset for the purpose of debugging the code. Not part of the required steps in the pipeline.\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "subset_indices = torch.arange(0, 300)\n",
    "sample_train = Subset(train_dataset, subset_indices)\n",
    "\n",
    "train_loader = DataLoader(sample_train, \n",
    "                        batch_size=config[\"train_BatchSize\"], \n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa94a8cf-a055-4dba-b0d6-bb5f82d7e483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "show_random_patches(train_dataset, sample_num=3, rgb_bands=(3, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabf4d90-8398-4ea0-8920-d76f710b051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "# Check the distribution of categories in a dataset\n",
    "labels_count = get_labels_distribution(train_dataset)\n",
    "plot_labels_distribution(labels_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b898a60-29f3-4ec8-99fb-dee3bdabfb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1-2\n",
    "# Batchify the training dataset and put it on the defined 'Device'.\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=config[\"train_BatchSize\"], \n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ec53bd-77b4-4282-b062-66928cdaca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1-3\n",
    "# Read the input dataset for validation, pre-process it and generate a 'torch.utils.Dataset' object \n",
    "# that can be accepted to get loaded into the model.\n",
    "val_dataset = CropData(src_dir=config[\"src_dir\"],\n",
    "                       usage=\"validation\",\n",
    "                       dataset_name=config[\"train_dataset_name\"],\n",
    "                       csv_path=config[\"val_csv_path\"],\n",
    "                       apply_normalization=config[\"apply_normalization\"],\n",
    "                       normal_strategy=config[\"normal_strategy\"],\n",
    "                       stat_procedure=config[\"stat_procedure\"],\n",
    "                       global_stats=config[\"global_stats\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a391e01-11c0-4196-83f4-0a9ec79f5fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN\n",
    "# Making a subset of the evaluation dataset for the purpose of debugging the code. Not part of the required steps in the pipeline.\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "subset_indices = torch.arange(0, 100)\n",
    "sample_val = Subset(val_dataset, subset_indices)\n",
    "\n",
    "val_loader = DataLoader(sample_val, \n",
    "                        batch_size=config[\"val_test_BatchSize\"], \n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b87734-63bc-4495-b63a-b9af626d4dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL\n",
    "# Check the distribution of categories in a dataset\n",
    "labels_count = get_labels_distribution(val_dataset)\n",
    "plot_labels_distribution(labels_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f9432a-dbf3-4981-ae2c-144aa5f46ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1-4\n",
    "# Batchify the validation dataset and put it on the defined 'Device'.\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                        batch_size=config[\"val_test_BatchSize\"], \n",
    "                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46523fb6-c853-48c4-b671-1adc41997759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2\n",
    "# Initialize the model\n",
    "model = Unet(n_classes=config[\"n_classes\"], \n",
    "             in_channels=config[\"input_channels\"], \n",
    "             use_skipAtt=config[\"use_skipAtt\"],\n",
    "             filter_config=config[\"filter_config\"],\n",
    "             dropout_rate=config[\"train_dropout_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b69356-5b9d-4004-8db7-0ea187e3d9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3\n",
    "# Compile the model as vanilla or pre-trained object with 'fit', 'save', 'accuracy_evaluation'\n",
    "# and 'inference' methods. It also has checkpointing and resume capabilities and generates \n",
    "# tensorboard reports and graphs.\n",
    "compiled_model = ModelCompiler(model,\n",
    "                               working_dir=config[\"working_dir\"],\n",
    "                               out_dir=config[\"out_dir\"],\n",
    "                               num_classes=config[\"n_classes\"],\n",
    "                               inch=config[\"input_channels\"],\n",
    "                               class_mapping=config[\"class_mapping\"],\n",
    "                               gpu_devices=config[\"gpuDevices\"],\n",
    "                               model_init_type=config[\"init_type\"], \n",
    "                               params_init=config[\"params_init\"],\n",
    "                               freeze_params=config[\"freeze_params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c363c960-0171-4f7a-a678-82dc11c75f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4\n",
    "# train and validate the model for the defined number of epochs\n",
    "compiled_model.fit(train_loader,\n",
    "                   val_loader, \n",
    "                   epochs=config[\"epochs\"], \n",
    "                   optimizer_name=config[\"optimizer\"], \n",
    "                   lr_init=config[\"LR\"],\n",
    "                   lr_policy=config[\"LR_policy\"], \n",
    "                   criterion=config[\"criterion\"], \n",
    "                   momentum=config[\"momentum\"],\n",
    "                   resume=config[\"resume\"],\n",
    "                   resume_epoch=config[\"resume_epoch\"],\n",
    "                   **config[\"lr_prams\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e128e11f-149e-4a47-a4d4-9f31880b08be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5\n",
    "# Same model parameters\n",
    "compiled_model.save(save_object=\"params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef3f31e-6976-4e2e-a659-c03e636e630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6\n",
    "# Generate the accuracy metrics\n",
    "compiled_model.accuracy_evaluation(val_loader, filename=config[\"val_metric_fname\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a8d74-eadd-41b6-b9ed-3bdcaaa240c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941320c7-ebee-4ce5-b3b0-3ce3c938b0a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
