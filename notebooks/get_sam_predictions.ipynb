{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c10adec7-e5a4-4400-853c-fb9f58baffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3' #cannot work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf7aaf3c-44a2-464c-9ab4-4154045e89b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import torch\n",
    "import rasterio\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24ace2ab-d9e4-4652-bef5-f48dc775154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import label\n",
    "from PIL import Image\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ad13b8-e45f-4e97-a0c0-691c9e1b9882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifffile import imwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eca6df4b-30a2-40df-b4a6-6cadbc49f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamModel, SamProcessor\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-base\").to(0)\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcc555f2-248f-49e3-9162-c8ba09efd159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/SAM/Fine_tune_SAM_(segment_anything)_on_a_custom_dataset.ipynb\n",
    "def get_bounding_box(ground_truth_map):\n",
    "  # get bounding box from mask\n",
    "  z, y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "  x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "  y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "  bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "  return bbox\n",
    "\n",
    "def get_bboxes(mask_tensor):\n",
    "    crop_bboxes = []\n",
    "    building_bboxes = []\n",
    "    building_groundtruth = np.where(mask_tensor % 2 == 1, 1, 0)[0,:,:,:]\n",
    "    field_groundtruth = np.where(mask_tensor==2, 1, 0)[0,:,:,:]\n",
    "\n",
    "    building_masks, building_num_labels = label(building_groundtruth)\n",
    "    for i in range(1, building_num_labels + 1):\n",
    "        building_object_mask = np.where(building_masks == i, 1, 0)\n",
    "        bbox = get_bounding_box(building_object_mask)\n",
    "        building_bboxes.append(bbox)\n",
    "        \n",
    "    field_masks, field_num_labels = label(field_groundtruth)\n",
    "    for i in range(1, field_num_labels + 1):\n",
    "        field_object_mask = np.where(field_masks == i, 1, 0)\n",
    "        bbox = get_bounding_box(field_object_mask)\n",
    "        crop_bboxes.append(bbox)\n",
    "    return building_bboxes, crop_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e322a145-27d6-44b3-9447-3578fe8fcaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagedir = '/home/data/kenya/images/'\n",
    "maskdir = '/home/workdir/kenya_output_skipatt/hardened_prob/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f224856f-4fd4-45be-bc88-b5bfdc548026",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_paths = [Path(maskdir).joinpath(i[9:]) for i in os.listdir(maskdir) if i.endswith('.tif')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "294ad1ae-889f-44b0-9418-1b91db1f72c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/workdir/kenya_output_skipatt/hardened_prob/kenol2_1330.tif')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_paths[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7b924004-c700-4013-8420-7b223247b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteData(Dataset):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 imagedir, maskdir):\n",
    "        self.image_dir = Path(imagedir)\n",
    "        self.mask_dir = Path(maskdir)\n",
    "        self.tif_paths = self._get_tif_paths()\n",
    "        self.mask_paths = self._get_mask_paths()\n",
    "\n",
    "\n",
    "    def _get_tif_paths(self):\n",
    "        tif_paths = [self.image_dir.joinpath(i[9:]) for i in os.listdir(self.mask_dir) if i.endswith('.tif')]\n",
    "        return tif_paths\n",
    "\n",
    "    def _get_mask_paths(self):\n",
    "        mask_paths = [self.mask_dir.joinpath(i) for i in os.listdir(self.mask_dir) if i.endswith('.tif')]\n",
    "        return mask_paths\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tif_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        def read_tif_as_np_array(path):\n",
    "            with rasterio.open(path) as src:\n",
    "                    return src.read()\n",
    "                \n",
    "        # Read in merged tif as ground truth\n",
    "        groundtruth = read_tif_as_np_array(self.mask_paths[index])\n",
    "        groundtruth = torch.tensor(groundtruth, dtype=torch.uint8)\n",
    "        image = read_tif_as_np_array(self.tif_paths[index])\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        chip_name = str(self.tif_paths[index]).split('/')[-1]\n",
    "        \n",
    "        return chip_name, groundtruth, image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9af973df-0bda-4920-aea1-b3787380bba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'driver': 'GTiff', 'dtype': 'int16', 'nodata': None, 'width': 512, 'height': 512, 'count': 1, 'crs': CRS.from_epsg(32737), 'transform': Affine(0.08780717699164597, 0.0, 292067.5720761125,\n",
      "       0.0, -0.08780717699119124, 9895222.627567634), 'blockysize': 8, 'tiled': False, 'interleave': 'band'}\n"
     ]
    }
   ],
   "source": [
    "with rasterio.open('/home/workdir/kenya_output_reclass/hardened_prob/crisp_id_kenol1_1007.tif') as src:\n",
    "    print(src.profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6d313e48-d4ca-490d-8c11-f5e34f6b8e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "image_dataset = SatelliteData(imagedir = '/home/data/kenya/images/', maskdir = '/home/workdir/kenya_output_skipatt/hardened_prob/')\n",
    "\n",
    "# data loader\n",
    "image_loader = DataLoader(image_dataset, \n",
    "                          batch_size  = 1, \n",
    "                          shuffle     = False)\n",
    "\n",
    "# display images\n",
    "for batch_idx, inputs in enumerate(image_loader):\n",
    "    print(inputs[2].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "47cb6317-cdf8-4abf-b9e3-30c2f22976f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = pathlib.Path('/home/workdir/kenya_output_skipatt/sam_predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "63cfbd2c-8456-4992-8baa-08d0e732b5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 205/205 [01:33<00:00,  2.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# GET JUST BUILDINGS\n",
    "for inputs in tqdm(image_loader):\n",
    "    path, pred, image = inputs\n",
    "    building_bboxes, crop_bboxes = get_bboxes(pred.numpy())\n",
    "    if crop_bboxes:\n",
    "        crop_inputs = processor(image[0,:3,:,:], input_boxes=[crop_bboxes], return_tensors=\"pt\").to(0)\n",
    "        crop_outputs = model(**crop_inputs)\n",
    "        crop_masks = processor.image_processor.post_process_masks(crop_outputs.pred_masks.cpu(), crop_inputs[\"original_sizes\"].cpu(), crop_inputs[\"reshaped_input_sizes\"].cpu())\n",
    "\n",
    "        del crop_inputs, crop_outputs\n",
    "    if building_bboxes:\n",
    "        building_inputs = processor(image[0,:3,:,:], input_boxes=[building_bboxes], return_tensors=\"pt\").to(0)\n",
    "        building_outputs = model(**building_inputs)\n",
    "        building_masks = processor.image_processor.post_process_masks(building_outputs.pred_masks.cpu(), building_inputs[\"original_sizes\"].cpu(), building_inputs[\"reshaped_input_sizes\"].cpu())\n",
    "    \n",
    "        del building_inputs, building_outputs\n",
    "\n",
    "    crops = pred[0,:,:,:]\n",
    "    \n",
    "    if building_bboxes:\n",
    "        building_mask = torch.any(building_masks[0], 0)[:1,:,:]\n",
    "        building_binary = torch.where(building_mask, 1, 0)\n",
    "        crop_mask = torch.where(crops==2, 2, 0)\n",
    "        sam_mask = torch.where(building_binary==1, 1, crop_mask).numpy()\n",
    "    else:\n",
    "        sam_mask = crops.numpy()\n",
    "        \n",
    "    imwrite(save_dir / path[0], sam_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "eb536c64-2048-420c-afb2-56d3362c8cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 205/205 [01:34<00:00,  2.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# GET CROPS AND BUILDINGS\n",
    "for inputs in tqdm(image_loader):\n",
    "    path, pred, image = inputs\n",
    "    building_bboxes, crop_bboxes = get_bboxes(pred.numpy())\n",
    "    if crop_bboxes:\n",
    "        crop_inputs = processor(image[0,:3,:,:], input_boxes=[crop_bboxes], return_tensors=\"pt\").to(0)\n",
    "        crop_outputs = model(**crop_inputs)\n",
    "        crop_masks = processor.image_processor.post_process_masks(crop_outputs.pred_masks.cpu(), crop_inputs[\"original_sizes\"].cpu(), crop_inputs[\"reshaped_input_sizes\"].cpu())\n",
    "\n",
    "        del crop_inputs, crop_outputs\n",
    "    if building_bboxes:\n",
    "        building_inputs = processor(image[0,:3,:,:], input_boxes=[building_bboxes], return_tensors=\"pt\").to(0)\n",
    "        building_outputs = model(**building_inputs)\n",
    "        building_masks = processor.image_processor.post_process_masks(building_outputs.pred_masks.cpu(), building_inputs[\"original_sizes\"].cpu(), building_inputs[\"reshaped_input_sizes\"].cpu())\n",
    "    \n",
    "        del building_inputs, building_outputs\n",
    "\n",
    "    crops = pred[0,:,:,:]\n",
    "    \n",
    "    if building_bboxes and crop_bboxes:\n",
    "        building_mask = torch.any(building_masks[0], 0)[:1,:,:]\n",
    "        building_binary = torch.where(building_mask, 1, 0)\n",
    "        crop_mask = torch.any(crop_masks[0], 0)[:1,:,:]\n",
    "        crop_binary = torch.where(crop_mask, 2, 0)\n",
    "        sam_mask = torch.where(building_binary==1, 1, crop_binary).numpy()\n",
    "    elif crop_bboxes:\n",
    "        crop_mask = torch.any(crop_masks[0], 0)[:1,:,:]\n",
    "        sam_mask = torch.where(crop_mask, 2, 0).numpy()\n",
    "    else:\n",
    "        building_mask = torch.any(building_masks[0], 0)[:1,:,:]\n",
    "        sam_mask = torch.where(building_mask, 1, 0).numpy()\n",
    "        \n",
    "    imwrite(save_dir / path[0], sam_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2f670bd5-ee57-448c-85c6-9162fd13c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "unetdir = Path('/home/workdir/kenya_output_skipatt/hardened_prob/')\n",
    "imagedir = Path('/home/data/kenya/images/')\n",
    "maskdir = Path('/home/data/kenya/labels/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "00facbc0-8d7d-40ea-87ba-6915fb999ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 205/205 [00:01<00:00, 186.10it/s]\n"
     ]
    }
   ],
   "source": [
    "rasters = [i for i in os.listdir(save_dir) if i.endswith('.tif')]\n",
    "for p in tqdm(rasters):\n",
    "    with rasterio.open(unetdir / ('crisp_id_'+p)) as src:\n",
    "        profile = src.profile\n",
    "    with rasterio.open(save_dir / p) as src:\n",
    "        array = src.read()\n",
    "    with rasterio.open((save_dir / p), 'w', **profile) as dst:\n",
    "        dst.write(array.astype(rasterio.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b1e1a7-9d68-43fc-a380-8e801d103bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
