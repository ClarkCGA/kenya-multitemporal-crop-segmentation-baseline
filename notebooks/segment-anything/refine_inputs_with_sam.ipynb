{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c10adec7-e5a4-4400-853c-fb9f58baffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3' #cannot work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf7aaf3c-44a2-464c-9ab4-4154045e89b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "import torch\n",
    "import rasterio\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24ace2ab-d9e4-4652-bef5-f48dc775154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import label\n",
    "from PIL import Image\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ad13b8-e45f-4e97-a0c0-691c9e1b9882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifffile import imwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba4a465-21bc-41d3-b674-7e2f7ce70be4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eca6df4b-30a2-40df-b4a6-6cadbc49f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SamModel, SamProcessor\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(0)\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcc555f2-248f-49e3-9162-c8ba09efd159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/SAM/Fine_tune_SAM_(segment_anything)_on_a_custom_dataset.ipynb\n",
    "def get_smaller_bounding_box(ground_truth_map):\n",
    "  # get bounding box from mask\n",
    "  z, y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "  x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "  y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "  x_range = x_max-x_min\n",
    "  y_range = y_max-y_min\n",
    "  x_factor, y_factor = int(x_range/20), int(y_range/20)\n",
    "  bbox = [x_min+x_factor, \n",
    "          y_min+y_factor, \n",
    "          x_max-x_factor, \n",
    "          y_max-y_factor]\n",
    "\n",
    "  return bbox\n",
    "\n",
    "def get_bounding_box(ground_truth_map):\n",
    "  # get bounding box from mask\n",
    "  z, y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "  x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "  y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "  bbox = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "  return bbox\n",
    "\n",
    "def get_bboxes(mask_tensor):\n",
    "    crop_bboxes = []\n",
    "    building_bboxes = []\n",
    "    building_groundtruth = np.where(mask_tensor % 2 == 1, 1, 0)[0,:,:,:]\n",
    "    field_groundtruth = np.where(mask_tensor==2, 1, 0)[0,:,:,:]\n",
    "\n",
    "    building_masks, building_num_labels = label(building_groundtruth)\n",
    "    for i in range(1, building_num_labels + 1):\n",
    "        building_object_mask = np.where(building_masks == i, 1, 0)\n",
    "        bbox = get_bounding_box(building_object_mask)\n",
    "        building_bboxes.append(bbox)\n",
    "        \n",
    "    field_masks, field_num_labels = label(field_groundtruth)\n",
    "    for i in range(1, field_num_labels + 1):\n",
    "        field_object_mask = np.where(field_masks == i, 1, 0)\n",
    "        bbox = get_smaller_bounding_box(field_object_mask)\n",
    "        crop_bboxes.append(bbox)\n",
    "    return building_bboxes, crop_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e322a145-27d6-44b3-9447-3578fe8fcaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagedir = '/home/data/kenya/images/'\n",
    "maskdir = '/home/data/kenya/labels/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f224856f-4fd4-45be-bc88-b5bfdc548026",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_paths = [Path(maskdir).joinpath(i) for i in os.listdir(maskdir) if i.endswith('.tif')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "294ad1ae-889f-44b0-9418-1b91db1f72c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/data/kenya/labels/kenol1_1929.tif')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_paths[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b924004-c700-4013-8420-7b223247b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteData(Dataset):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 imagedir, maskdir):\n",
    "        self.image_dir = Path(imagedir)\n",
    "        self.mask_dir = Path(maskdir)\n",
    "        self.tif_paths = self._get_tif_paths()\n",
    "        self.mask_paths = self._get_mask_paths()\n",
    "\n",
    "\n",
    "    def _get_tif_paths(self):\n",
    "        tif_paths = [self.image_dir.joinpath(i) for i in os.listdir(self.mask_dir) if i.endswith('.tif')]\n",
    "        return tif_paths\n",
    "\n",
    "    def _get_mask_paths(self):\n",
    "        mask_paths = [self.mask_dir.joinpath(i) for i in os.listdir(self.mask_dir) if i.endswith('.tif')]\n",
    "        return mask_paths\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tif_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        def read_tif_as_np_array(path):\n",
    "            with rasterio.open(path) as src:\n",
    "                    return src.read()\n",
    "                \n",
    "        # Read in merged tif as ground truth\n",
    "        groundtruth = read_tif_as_np_array(self.mask_paths[index])\n",
    "        groundtruth = torch.tensor(groundtruth, dtype=torch.uint8)\n",
    "        image = read_tif_as_np_array(self.tif_paths[index])\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        chip_name = str(self.tif_paths[index]).split('/')[-1]\n",
    "        \n",
    "        return chip_name, groundtruth, image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d313e48-d4ca-490d-8c11-f5e34f6b8e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "image_dataset = SatelliteData(imagedir = imagedir, maskdir = maskdir)\n",
    "\n",
    "# data loader\n",
    "image_loader = DataLoader(image_dataset, \n",
    "                          batch_size  = 1, \n",
    "                          shuffle     = False)\n",
    "\n",
    "# display images\n",
    "for batch_idx, inputs in enumerate(image_loader):\n",
    "    print(inputs[2].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47cb6317-cdf8-4abf-b9e3-30c2f22976f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = pathlib.Path('/home/data/kenya-sam/labels/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cfbd2c-8456-4992-8baa-08d0e732b5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET JUST BUILDINGS\n",
    "for inputs in tqdm(image_loader):\n",
    "    path, pred, image = inputs\n",
    "    building_bboxes, crop_bboxes = get_bboxes(pred.numpy())\n",
    "    if crop_bboxes:\n",
    "        crop_inputs = processor(image[0,:3,:,:], input_boxes=[crop_bboxes], return_tensors=\"pt\").to(0)\n",
    "        crop_outputs = model(**crop_inputs)\n",
    "        crop_masks = processor.image_processor.post_process_masks(crop_outputs.pred_masks.cpu(), crop_inputs[\"original_sizes\"].cpu(), crop_inputs[\"reshaped_input_sizes\"].cpu())\n",
    "\n",
    "        del crop_inputs, crop_outputs\n",
    "    if building_bboxes:\n",
    "        building_inputs = processor(image[0,:3,:,:], input_boxes=[building_bboxes], return_tensors=\"pt\").to(0)\n",
    "        building_outputs = model(**building_inputs)\n",
    "        building_masks = processor.image_processor.post_process_masks(building_outputs.pred_masks.cpu(), building_inputs[\"original_sizes\"].cpu(), building_inputs[\"reshaped_input_sizes\"].cpu())\n",
    "    \n",
    "        del building_inputs, building_outputs\n",
    "\n",
    "    crops = pred[0,:,:,:]\n",
    "    \n",
    "    if building_bboxes:\n",
    "        building_mask = torch.any(building_masks[0], 0)[:1,:,:]\n",
    "        building_binary = torch.where(building_mask, 1, 0)\n",
    "        crop_mask = torch.where(crops==2, 2, 0)\n",
    "        sam_mask = torch.where(building_binary==1, 1, crop_mask).numpy()\n",
    "    else:\n",
    "        sam_mask = crops.numpy()\n",
    "        \n",
    "    imwrite(save_dir / path[0], sam_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb536c64-2048-420c-afb2-56d3362c8cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/2041 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 15.74 GiB total capacity; 14.50 GiB already allocated; 384.62 MiB free; 14.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m crop_bboxes:\n\u001b[1;32m      6\u001b[0m     crop_inputs \u001b[38;5;241m=\u001b[39m processor(image[\u001b[38;5;241m0\u001b[39m,:\u001b[38;5;241m3\u001b[39m,:,:], input_boxes\u001b[38;5;241m=\u001b[39m[crop_bboxes], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     crop_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcrop_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     crop_masks \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mimage_processor\u001b[38;5;241m.\u001b[39mpost_process_masks(crop_outputs\u001b[38;5;241m.\u001b[39mpred_masks\u001b[38;5;241m.\u001b[39mcpu(), crop_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu(), crop_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreshaped_input_sizes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m crop_inputs, crop_outputs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/sam/modeling_sam.py:1361\u001b[0m, in \u001b[0;36mSamModel.forward\u001b[0;34m(self, pixel_values, input_points, input_labels, input_boxes, input_masks, image_embeddings, multimask_output, attention_similarity, target_embedding, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m vision_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1361\u001b[0m     vision_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1367\u001b[0m     image_embeddings \u001b[38;5;241m=\u001b[39m vision_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/sam/modeling_sam.py:1050\u001b[0m, in \u001b[0;36mSamVisionEncoder.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1046\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1047\u001b[0m         hidden_states,\n\u001b[1;32m   1048\u001b[0m     )\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1050\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/sam/modeling_sam.py:943\u001b[0m, in \u001b[0;36mSamVisionLayer.forward\u001b[0;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[1;32m    940\u001b[0m     height, width \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], hidden_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    941\u001b[0m     hidden_states, padding_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_partition(hidden_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[0;32m--> 943\u001b[0m hidden_states, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/sam/modeling_sam.py:843\u001b[0m, in \u001b[0;36mSamVisionAttention.forward\u001b[0;34m(self, hidden_states, output_attentions)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;66;03m# q, k, v with shape (batch_size * nHead, height * width, channel)\u001b[39;00m\n\u001b[1;32m    841\u001b[0m query, key, value \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m3\u001b[39m, batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads, height \u001b[38;5;241m*\u001b[39m width, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 843\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_rel_pos:\n\u001b[1;32m    846\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_decomposed_rel_pos(\n\u001b[1;32m    847\u001b[0m         attn_weights, query, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrel_pos_h, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrel_pos_w, (height, width), (height, width)\n\u001b[1;32m    848\u001b[0m     )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 15.74 GiB total capacity; 14.50 GiB already allocated; 384.62 MiB free; 14.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# GET CROPS AND BUILDINGS\n",
    "for inputs in tqdm(image_loader):\n",
    "    path, pred, image = inputs\n",
    "    building_bboxes, crop_bboxes = get_bboxes(pred.numpy())\n",
    "    if crop_bboxes:\n",
    "        crop_inputs = processor(image[0,:3,:,:], input_boxes=[crop_bboxes], return_tensors=\"pt\").to(0)\n",
    "        crop_outputs = model(**crop_inputs)\n",
    "        crop_masks = processor.image_processor.post_process_masks(crop_outputs.pred_masks.cpu(), crop_inputs[\"original_sizes\"].cpu(), crop_inputs[\"reshaped_input_sizes\"].cpu())\n",
    "\n",
    "        del crop_inputs, crop_outputs\n",
    "        \n",
    "    if building_bboxes:\n",
    "        building_inputs = processor(image[0,:3,:,:], input_boxes=[building_bboxes], return_tensors=\"pt\").to(0)\n",
    "        building_outputs = model(**building_inputs)\n",
    "        building_masks = processor.image_processor.post_process_masks(building_outputs.pred_masks.cpu(), building_inputs[\"original_sizes\"].cpu(), building_inputs[\"reshaped_input_sizes\"].cpu())\n",
    "    \n",
    "        del building_inputs, building_outputs\n",
    "\n",
    "    crops = pred[0,:,:,:]\n",
    "    \n",
    "    if building_bboxes and crop_bboxes:\n",
    "        building_mask = torch.any(building_masks[0], 0)[:1,:,:]\n",
    "        building_binary = torch.where(building_mask, 1, 0)\n",
    "        crop_mask = torch.any(crop_masks[0], 0)[:1,:,:]\n",
    "        crop_binary = torch.where(crop_mask, 2, 0)\n",
    "        sam_mask = torch.where(building_binary==1, 1, crop_binary).numpy()\n",
    "    elif crop_bboxes:\n",
    "        crop_mask = torch.any(crop_masks[0], 0)[:1,:,:]\n",
    "        sam_mask = torch.where(crop_mask, 2, 0).numpy()\n",
    "    else:\n",
    "        building_mask = torch.any(building_masks[0], 0)[:1,:,:]\n",
    "        sam_mask = torch.where(building_mask, 1, 0).numpy()\n",
    "        \n",
    "    imwrite(save_dir / path[0], sam_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f670bd5-ee57-448c-85c6-9162fd13c063",
   "metadata": {},
   "outputs": [],
   "source": [
    "unetdir = Path('/home/workdir/solar_test_output/hardened_prob/')\n",
    "imagedir = Path('/home/data/kenya-sam/images/')\n",
    "maskdir = Path('/home/data/kenya-sam/labels/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "00facbc0-8d7d-40ea-87ba-6915fb999ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 111.66it/s]\n"
     ]
    }
   ],
   "source": [
    "rasters = [i for i in os.listdir(save_dir) if i.endswith('.tif')]\n",
    "for p in tqdm(rasters):\n",
    "    with rasterio.open(unetdir / ('crisp_id_'+p)) as src:\n",
    "        profile = src.profile\n",
    "    with rasterio.open(save_dir / p) as src:\n",
    "        array = src.read()\n",
    "    with rasterio.open((save_dir / p), 'w', **profile) as dst:\n",
    "        dst.write(array.astype(rasterio.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89b1e1a7-9d68-43fc-a380-8e801d103bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2041/2041 [00:13<00:00, 146.86it/s]\n"
     ]
    }
   ],
   "source": [
    "rasters = [i for i in os.listdir(save_dir) if i.endswith('.tif')]\n",
    "for p in tqdm(rasters):\n",
    "    with rasterio.open(maskdir / p ) as src:\n",
    "        profile = src.profile\n",
    "    with rasterio.open(save_dir / p) as src:\n",
    "        array = src.read()\n",
    "    with rasterio.open((save_dir / p), 'w', **profile) as dst:\n",
    "        dst.write(array.astype(rasterio.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7519b0b4-6c6c-4278-ab33-69e00b57b943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
